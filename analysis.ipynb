{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apd_utils as apd\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate a referenced dataset and a random sample and perform DBSCAN\n",
    "The initial step involves generating a referenced dataset and establishing a generic analytical workflow. To evaluate a sample, only five steps are necessary, as outlined below. It is important to note that in this case, a non-adulterated sample is generated, which means that the label predicted by DBSCAN for the sample should be identical to the labels of the referenced samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels for referenced data: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Label for the non-adulterated sample: 0\n"
     ]
    }
   ],
   "source": [
    "# 1. Generate a referenced dataset to represent authentic products\n",
    "data_ref = apd.generate_ref_data(num_sample=50, num_analyte=200, seed=32)\n",
    "\n",
    "# 2. Generate a non-adulterated random sample\n",
    "sample_ref = apd.generate_random_sample(data_ref, adulterated=False, seed=64)\n",
    "\n",
    "# 3. Combine referenced data and the random sample\n",
    "data_comb = np.concatenate((data_ref, sample_ref), axis=0)\n",
    "\n",
    "# 4. Scale the data  \n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data_comb)\n",
    "\n",
    "# 5. Perform DBSCAN on scaled data\n",
    "dbscan = DBSCAN(eps=20).fit(scaled_data)\n",
    "\n",
    "print(f\"Labels for referenced data: {dbscan.labels_[0:30]}\")\n",
    "print(f\"Label for the non-adulterated sample: {dbscan.labels_[-1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate some adulterated samples and perform DBSCAN \n",
    "In this case, we generated three adulterated samples with varying numbers of altered analytes in `num_analyte`. By default, each analyte concentration is reduced to a value between the 0th and 5th percentile of that analyte from the referenced data. For instance, the first sample consists of 20 random analytes, with their concentration values being altered to values below the 5th percentile. This concentration manipulation can be specified in `conc_percentile` (e.g., `conc_percentile = (0,5)`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels for referenced data: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Label for 3 adulterated samples: [ 0 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# Generate a few adulterated samples\n",
    "sample_adu1, analyte_idx1 = apd.generate_random_sample(data_ref, adulterated=True, num_analyte=20,\n",
    "                                                       analyte_select_method=\"random\", seed=16)\n",
    "\n",
    "sample_adu2, analyte_idx2  = apd.generate_random_sample(data_ref, adulterated=True, num_analyte=25,\n",
    "                                                        analyte_select_method=\"random\", seed=64)\n",
    "\n",
    "sample_adu3, analyte_idx3 = apd.generate_random_sample(data_ref, adulterated=True, num_analyte=30,\n",
    "                                                       analyte_select_method=\"random\", seed=135)\n",
    "\n",
    "# Combine samples \n",
    "data_comb = np.vstack((data_ref, sample_adu1, sample_adu2, sample_adu3))\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data_comb)\n",
    "\n",
    "# Perform DBSCAN on scaled data\n",
    "dbscan = DBSCAN(eps=20).fit(scaled_data)\n",
    "\n",
    "print(f\"Labels for referenced data: {dbscan.labels_[0:50]}\")\n",
    "print(f\"Label for 3 adulterated samples: {dbscan.labels_[-3:]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results demonstrate that the 2nd and 3rd adulterated samples were assigned a label of -1, indicating that DBSCAN detected them as noisy data. Conversely, the 1st adulterated sample received a label of 0, identical to the other referenced samples, implying that it went undetected by the model. If you set the random seed to `None` and rerun the script, you may notice occasional changes in the predicted labels. This variability suggests that the number of affected analytes can influence the detection outcomes. In fact, several factors can influence the results, which we will discuss in the following sections."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine tune DBSCAN parameter \"eps\" to optimize detection \n",
    "The parameter **eps** represents the maximum distance between two samples for them to be considered in each other's neighborhood. It plays a crucial role in the predicted results of your data. Due to variations in the referenced data such as the number of samples, analytes, and the mean and standard deviation of analyte quantitative data, the distances between data points in multi-dimensional spaces can differ. Consequently, fine-tuning **eps** becomes necessary to obtain meaningful results.\n",
    "\n",
    "In this case, if **eps** is set too small, some referenced data points might be labeled as noisy data. Conversely, if **eps** is too large, adulterated samples may be grouped together with the referenced data as a single cluster. To address this, I demonstrate a method to identify an optimal **eps** value by incrementally increasing it by 0.1. This process continues until the predicted labels of the referenced data all become identical (all zeros). By finding this ideal **eps**, we can achieve more accurate clustering results and enhance the effectiveness of the detection methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN eps: 19.60000000000003\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data_ref)\n",
    "\n",
    "eps = 10.0\n",
    "eps_max = 30.0\n",
    "\n",
    "while eps < eps_max:\n",
    "    eps += 0.1\n",
    "    dbscan = DBSCAN(eps=eps).fit(scaled_data)\n",
    "    if np.sum(dbscan.labels_) == 0:\n",
    "        break\n",
    "\n",
    "print(f\"DBSCAN eps: {eps}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate model performance under various scenarios\n",
    "In this section, I developed a function to iteratively detect random adulterated samples, enabling the calculation of model accuracies across various scenarios. Notably, the function `generate_random_sample()` is utilized, and three of its arguments are included here for direct usage. This allows us to pass the necessary arguments into the function and evaluate the model's performance effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_accuracy(\n",
    "        data, \n",
    "        iterations=500, \n",
    "        num_analyte=30, \n",
    "        analyte_select_method=\"random\", \n",
    "        conc_percentile=(0,5),\n",
    "        eps=10, \n",
    "        msg=True\n",
    "    ):\n",
    "    # number of samples detected as adulterated\n",
    "    cnt = 0\n",
    "\n",
    "    for i in tqdm(range(iterations)):\n",
    "\n",
    "        # generate an adulterant\n",
    "        sample, analyte_idx = apd.generate_random_sample(data=data, adulterated=True, num_analyte=num_analyte,\n",
    "                                                         analyte_select_method=analyte_select_method,\n",
    "                                                         conc_percentile=conc_percentile)\n",
    "        \n",
    "        # combine samples\n",
    "        data_comb = np.vstack((data, sample))\n",
    "\n",
    "        # scale the data\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(data_comb)\n",
    "\n",
    "        # perform DBSCAN \n",
    "        dbscan = DBSCAN(eps=eps).fit(scaled_data)\n",
    "\n",
    "        if dbscan.labels_[-1] == -1:\n",
    "            cnt += 1\n",
    "\n",
    "    if msg:\n",
    "        print(f\"{cnt} out of {iterations} adulterated samples were identified. Accuracy = {round(cnt/iterations, 3)}\")\n",
    "\n",
    "    return round(cnt/iterations, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subsequent runs assess the model's performance with three different numbers of altered analytes (n = 30, 35, and 40), where their values are set between the 0th and 5th percentiles. Analytes are randomly selected for each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73842349c4554c04b0394932b128b5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341 out of 500 adulterated samples were identified. Accuracy = 0.682\n"
     ]
    }
   ],
   "source": [
    "accu = detection_accuracy(data_ref, iterations=500, num_analyte=30, analyte_select_method=\"random\", conc_percentile=(0,5), eps=19.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217c7aaad6e043c9aacd1725b2e06ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399 out of 500 adulterated samples were identified. Accuracy = 0.798\n"
     ]
    }
   ],
   "source": [
    "accu = detection_accuracy(data_ref, iterations=500, num_analyte=35, analyte_select_method=\"random\", conc_percentile=(0,5), eps=19.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71aa8e85db294a089d60dc04d2d8769e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471 out of 500 adulterated samples were identified. Accuracy = 0.942\n"
     ]
    }
   ],
   "source": [
    "accu = detection_accuracy(data_ref, iterations=500, num_analyte=40, analyte_select_method=\"random\", conc_percentile=(0,5), eps=19.6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the accuracies demonstrated an upward trend as the number of altered analytes increased. It became easier for the model to identify adulterated samples with a higher number of affected analytes. In this particular scenario, the model achieved an accuracy exceeding 0.9 when approximately 40 out of the 200 analytes in a product were altered. This highlights the model's capability to accurately detect adulteration when a significant proportion of analytes are modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
